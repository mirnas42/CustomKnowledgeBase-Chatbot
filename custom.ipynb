{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Pinecone as PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"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\")\n",
    "PINECONE_API_KEY = os.getenv(\"pcsk_PaJKb_AmjfE444nDhvQA1TMCZHEmj9nnj3rixEX976ESeS78bA9C6r7cmcLYotJXVhHT5\")\n",
    "PINECONE_INDEX_NAME = \"custom-vectordb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents\n",
    "pdf_path = \"documents/ABOUT THE WORKCOHOL ORGANIZATION.pdf\"\n",
    " # Make sure this file is present in the working directory\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the pages into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face Inference API embeddings\n",
    "embedding_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    api_key=\"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\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Create an instance of the Pinecone class\n",
    "pc = Pinecone(api_key=\"pcsk_PaJKb_AmjfE444nDhvQA1TMCZHEmj9nnj3rixEX976ESeS78bA9C6r7cmcLYotJXVhHT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone and create the index connection\n",
    "pc = Pinecone(api_key=\"pcsk_5GaY1y_8sAoHhVACpt45xUSYZFaifSzCeAoMr8HZbuexftJXE6X4bKxP24NLHTW7MAnZTi\")\n",
    "pinecone_index = pc.Index(\"custom-vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectors = embedding_model.embed_query(\"how are you\")\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    }
   ],
   "source": [
    "# Create a Pinecone vector store from the documents\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding_model,\n",
    "    index_name=\"custom-vectordb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM INTEGRATION\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def ask_question(query: Query):\n",
    "    try:\n",
    "        answer = qa.run(query.question)\n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    }
   ],
   "source": [
    "if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "    # Create new index from docs\n",
    "    vector_store = PineconeVectorStore.from_documents(\n",
    "        docs,\n",
    "        embedding_model,\n",
    "        index_name=PINECONE_INDEX_NAME\n",
    "    )\n",
    "else:\n",
    "    # Connect to existing index\n",
    "    vector_store = PineconeVectorStore.from_existing_index(\n",
    "        index_name=PINECONE_INDEX_NAME,\n",
    "        embedding=embedding_model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 53 pages and split into 117 chunks.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(f\"Loaded {len(pages)} pages and split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['d:\\\\Final year project 1\\\\KB DOC\\\\customkbvenv\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can contact the HR department by sending an email to murali@workcohol.com or by visiting the careers page on the Workcohol Organizations website. Alternatively, you can find Murali Thangavel on LinkedIn for more information.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# Updated embeddings import\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pinecone retriever setup\n",
    "vectorstore = Pinecone.from_existing_index(\n",
    "    index_name=\"custom-vectordb\",\n",
    "    embedding=embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# NEW: Using `invoke()` instead of `get_relevant_documents()`\n",
    "question = \"How to contact HR department?\"\n",
    "docs = retriever.invoke(question)  # No warning\n",
    "\n",
    "# Combine retrieved chunks\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Prompt template\n",
    "full_prompt = f\"\"\"You are a helpful assistant. Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Setup Inference Client with Mistral-7B\n",
    "client = InferenceClient(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",  # Model to use\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "# Generate answer using `predict` method\n",
    "response = client.text_generation(\n",
    "    prompt=full_prompt,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.3,\n",
    "    do_sample=True,\n",
    "    return_full_text=False  # Optional: if you only want the completion\n",
    ")\n",
    "\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "\n",
    "if sys.platform.startswith('win') and sys.version_info >= (3, 8):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customkbvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
